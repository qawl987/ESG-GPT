{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "sys.path.append(\"/home/kai/muku_esg/ESG-BERT/\")\n",
    "from allgrammer import grammer\n",
    "from pdfminer.high_level import extract_text\n",
    "%cd /home/kai/muku_esg/ESG-BERT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]', '', text)\n",
    "    text = re.sub(r'[\\x0C]', r\"\\n\\n\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = 'taiwan_pdf'\n",
    "# csv_folder = 'predict_taiwan'\n",
    "# word_cnt = 'eight_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'american_pdf'\n",
    "csv_folder = 'predict_american'\n",
    "word_cnt = 'eight_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, folder, file_name):\n",
    "    x = x.replace('\\n', ' ')\n",
    "    x = re.sub(r'[!\\\"#$%&\\'()*\\+,-.\\/:;<=>?@\\[\\\\\\]^_`{|}~]', '', x)\n",
    "    x = x.replace('\\t', ' ')\n",
    "    x = x.strip()\n",
    "    if len(x.split()) < 8:\n",
    "        return None\n",
    "    else:\n",
    "        return grammer(x, folder, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '06_bxp_1_65'\n",
    "text = extract_text(f\"./{folder}/{file_name}.pdf\")\n",
    "text = ascii(text)\n",
    "text = text.strip('\\\"')\n",
    "paragraph = re.split(r'\\n\\s*\\n', text)\n",
    "constant_preprocess = partial(preprocess, folder=folder, file_name=file_name)\n",
    "paragraph = map(constant_preprocess, paragraph)\n",
    "paragraph = list(paragraph)\n",
    "# for i in range(len(paragraph)):\n",
    "#     print(paragraph[i])\n",
    "y = []\n",
    "for i in range(len(paragraph)):\n",
    "    if paragraph[i] != None:\n",
    "        x = paragraph[i].split()\n",
    "        fifthlist = x[:300]\n",
    "        fifthstr = ' '.join(fifthlist)\n",
    "        y.append(fifthstr)\n",
    "paragraph = y\n",
    "for i in range(len(paragraph)):\n",
    "    print(paragraph[i])\n",
    "# paragraph = [x for x in paragraph if x is not None]\n",
    "# paragraph = [x[:500] for x in paragraph if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd predict_american\n",
    "with open(\"./output.txt\", \"w\") as file:\n",
    "    file.truncate()\n",
    "with open(\"./paragraph.txt\", \"w\") as file:\n",
    "    file.truncate()\n",
    "for i in range(len(paragraph)):\n",
    "    # Open a file in write mode\n",
    "    with open(\"./paragraph.txt\", \"w\") as f:\n",
    "        # Write the strings to the file\n",
    "        f.writelines(paragraph[i])\n",
    "    # os.system(f'curl -X POST http://127.0.0.1:8080/predictions/bert -T paragraph.txt >> output.txt');\n",
    "    os.system(f'curl -X POST http://127.0.0.1:8090/predictions/bert -T paragraph.txt >> output.txt')\n",
    "    os.system(f'echo >> output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = pd.read_csv('./predict_file/output.txt', names=['labels'])\n",
    "with open('./output.txt', 'r') as f:\n",
    "    labels = f.readlines()\n",
    "    labels = [x.strip('\\n') for x in labels]\n",
    "ans = pd.DataFrame({'paragraph': paragraph, 'predict_label': labels})\n",
    "ans.to_csv(f'../{csv_folder}/{word_cnt}/{file_name}.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['15_msi_1_97.pdf',\n",
    "   '16_gigabyte_1_74.pdf', '17_nanya_1_154.pdf', '18_novatek_1_140.pdf', '19_mediatek_1_97.pdf', '20_asia_1_119.pdf', \n",
    "   '21_cathay_1_52.pdf', '22_cht_1_79.pdf', '23_fubon_1_89.pdf', '24_mega_1_68.pdf']\n",
    "for file_name in file_names:\n",
    "    file_name = file_name.strip('.pdf')\n",
    "    os.chdir('/home/kai/muku_esg/ESG-BERT')\n",
    "    text = extract_text(f\"./{folder}/{file_name}.pdf\")\n",
    "    text = ascii(text)\n",
    "    text = text.strip('\\\"')\n",
    "    paragraph = re.split(r'\\n\\s*\\n', text)\n",
    "    constant_preprocess = partial(preprocess, folder=folder, file_name=file_name)\n",
    "    paragraph = map(constant_preprocess, paragraph)\n",
    "    paragraph = list(paragraph)\n",
    "    y = []\n",
    "    for i in range(len(paragraph)):\n",
    "        if paragraph[i] != None:\n",
    "            x = paragraph[i].split()\n",
    "            fifthlist = x[:300]\n",
    "            fifthstr = ' '.join(fifthlist)\n",
    "            y.append(fifthstr)\n",
    "    paragraph = y\n",
    "    #   predict\n",
    "    os.chdir('./predict_american')\n",
    "    with open(\"./output.txt\", \"w\") as file:\n",
    "        file.truncate()\n",
    "    with open(\"./paragraph.txt\", \"w\") as file:\n",
    "        file.truncate()\n",
    "    for i in range(len(paragraph)):\n",
    "        # Open a file in write mode\n",
    "        with open(\"./paragraph.txt\", \"w\") as f:\n",
    "            # Write the strings to the file\n",
    "            f.writelines(paragraph[i])\n",
    "        # os.system(f'curl -X POST http://127.0.0.1:8080/predictions/bert -T paragraph.txt >> output.txt');\n",
    "        os.system(f'curl -X POST http://127.0.0.1:8080/predictions/bert -T paragraph.txt >> output.txt')\n",
    "        os.system(f'echo >> output.txt')\n",
    "    #   read output label and create csv\n",
    "    with open('./output.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "        labels = [x.strip('\\n') for x in labels]\n",
    "    ans = pd.DataFrame({'paragraph': paragraph, 'predict_label': labels})\n",
    "    ans.to_csv(f'../{csv_folder}/{word_cnt}/{file_name}.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir('./taiwan_pdf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taiwan = [\n",
    "'01_umc_1_154.pdf', '02_tsmc_1_210.pdf', '03_macronix_1_114.pdf', '04_esun_1_154.pdf', '05_eme_1_46.pdf',\n",
    " '06_asus_1_108.pdf', '07_acer_1_127.pdf', '08_witron_1_93.pdf', '09_honhai_1_22.pdf', '10_compal_1_169.pdf',\n",
    "  '11_quanta_1_123.pdf', '12_formosa_1_18.pdf', '13_csc_1_150.pdf', '14_qisda_1_134.pdf', '15_msi_1_97.pdf',\n",
    "   '16_gigabyte_1_74.pdf', '17_nanya_1_154.pdf', '18_novatek_1_140.pdf', '19_mediatek_1_97.pdf', '20_asia_1_119.pdf', \n",
    "   '21_cathay_1_52.pdf', '22_cht_1_79.pdf', '23_fubon_1_89.pdf', '24_mega_1_68.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\n",
    "'01_dow_1_85.pdf', '02_agilent_1_106.pdf', '03_amazon_1_79.pdf', '04_apple_1_72.pdf',\n",
    "'05_boeing_1_66.pdf', '06_bxp_1_65.pdf', '07_charles_1_50.pdf', '08_cisco_1_56.pdf', '09_citigroup_1_137.pdf', '10_cme_1_34.pdf', \n",
    "'11_colgate_1_84.pdf', '12_corning_1_71.pdf', '13_expeditor_1_37.pdf', '14_eei_1_80.pdf', '15_itt_1_44.pdf', \n",
    "'16_fedex_1_34.pdf', '17_firstscolar_1_57.pdf', '18_google_1_14.pdf', '19_intel_1_86.pdf', '20_jpmorgan_1_61.pdf', \n",
    "'21_microsoft_1_89.pdf', '22_rockwell_1_58.pdf', '23_ibm_1_49.pdf', '24_traveler_1_147.pdf', '25_visa_1_52.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = sorted(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Open the CSV file\n",
    "os.chdir('/home/kai/muku_esg/ESG-BERT/predict_taiwan/five_word')\n",
    "file_name = '08_witron_1_93'\n",
    "with open(f'./{file_name}.csv', 'r') as f:\n",
    "    # Create a CSV reader object\n",
    "    reader = csv.reader(f)\n",
    "    paragraphs = []\n",
    "    labels = []\n",
    "    for row in reader:\n",
    "        if row[0][:27] == '2021 Wistron ITS ESG Report' :\n",
    "            pass\n",
    "        else:\n",
    "            paragraphs.append(row[0])\n",
    "            labels.append(row[1])\n",
    "    ans = pd.DataFrame({'paragraph': paragraphs, 'predict_label': labels})\n",
    "    ans.to_csv(f'../five_new/{file_name}.csv', sep=',', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esgbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ab08fa76665a89038b50b331dd9223d4e470d2d4facccdbf60961d882d1011"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
