{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VScodeProject\\ESG-GPT\\esg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "class EsgBert(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EsgBert, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained('nbroad/ESG-BERT')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        logits = output[0]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsgDataset():\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids =  {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsgBertPredict():\n",
    "    def __init__(self, csv_source, csv_output, nation, file, hyper_parameters) -> None:\n",
    "        self.CSV_SOURCE = csv_source\n",
    "        self.CSV_OUTPUT = csv_output\n",
    "        self.NATION = nation\n",
    "        self.FILE = file\n",
    "        self.HP = hyper_parameters\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.df = None\n",
    "\n",
    "    def _read_csv(self):\n",
    "        self.df = pd.read_csv(f'{self.CSV_SOURCE}/{self.NATION}/{self.FILE}.csv')\n",
    "        x = self.df['paragraph']\n",
    "        return x\n",
    "    \n",
    "    def _tokenizer(self, x):\n",
    "        tokenizer = AutoTokenizer.from_pretrained('nbroad/ESG-BERT')\n",
    "        inference_encodings = tokenizer(x.to_list(), truncation=True, padding=True)\n",
    "        return inference_encodings\n",
    "    \n",
    "    def _get_dataset(self, inference_encodings):\n",
    "        dataset = EsgDataset(inference_encodings)\n",
    "        return dataset\n",
    "    \n",
    "    def _get_dataloader(self, inference) -> DataLoader:\n",
    "        inference_loader = DataLoader(inference, batch_size=self.HP['batch_size'], shuffle=True)\n",
    "        return inference_loader\n",
    "    \n",
    "    def _get_model(self) -> Tuple[EsgBert, torch.optim.Optimizer]:\n",
    "        model = EsgBert().to(device)\n",
    "        optim = AdamW(model.parameters(), lr=1e-5)\n",
    "        return model, optim\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, model: EsgBert, optim: torch.optim.Optimizer, inference_loader):\n",
    "        count = 0\n",
    "        model.eval()\n",
    "        output_label = []\n",
    "        loop = tqdm(inference_loader, leave=True)\n",
    "        for batch_id, batch in enumerate(loop):\n",
    "            optim.zero_grad()\n",
    "            inputs = batch\n",
    "            input_ids = inputs['input_ids'].to(self.device)\n",
    "            attention_mask = inputs['attention_mask'].to(self.device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            #  collect output into list\n",
    "            outputs = outputs.cpu().tolist()\n",
    "            output_label.extend(outputs)\n",
    "            if batch_id % 50 == 0 and batch_id != 0:\n",
    "                print(f'Epoch {batch_id}, count is {count}')\n",
    "        self._gen_csv(output_label)\n",
    "        \n",
    "    def _gen_csv(self, output_label):\n",
    "        self.df['label'] = output_label\n",
    "        self.df.to_csv(f'{self.CSV_OUTPUT}/{self.NATION}/esgbert_inference.csv', index=False)\n",
    "        \n",
    "    def main(self):\n",
    "        x = self._read_csv()\n",
    "        inference_encodings = self._tokenizer(x)\n",
    "        inference_dataset = self._get_dataset(inference_encodings)\n",
    "        inference_loader = self._get_dataloader(inference_dataset)\n",
    "        model, optim = self._get_model()\n",
    "        self.inference(model, optim, inference_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMETERS = {\n",
    "    'batch_size': 8,\n",
    "    'lr': 1e-5,\n",
    "    'epochs': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_SOURCE = '../../data/csv_source'\n",
    "CSV_OUTPUT = '../../data/csv_output'\n",
    "NATION = ''\n",
    "FILE = '4_apple_1_72'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 53/78 [00:03<00:01, 17.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, count is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.67it/s]\n"
     ]
    }
   ],
   "source": [
    "esgbert = EsgBertPredict(CSV_SOURCE, CSV_OUTPUT, NATION, FILE, HYPER_PARAMETERS)\n",
    "esgbert.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
