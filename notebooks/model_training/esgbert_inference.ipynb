{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "set_seed(777)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.nn import LogSoftmax\n",
    "class EsgBert(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(EsgBert, self).__init__()\n",
    "\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained('nbroad/ESG-BERT')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        logits = output[0]\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsgDataset():\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids =  {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsgBertPredict():\n",
    "    def __init__(self, csv_source, csv_output, nation, hyper_parameters) -> None:\n",
    "        self.CSV_SOURCE = csv_source\n",
    "        self.CSV_OUTPUT = csv_output\n",
    "        self.NATION = nation\n",
    "        self.HP = hyper_parameters\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.df = None\n",
    "\n",
    "    def _read_csv(self):\n",
    "        self.df = pd.read_csv(f'{self.CSV_SOURCE}/american/4_apple_1_72.csv')\n",
    "        x = self.df['paragraph']\n",
    "        return x\n",
    "    \n",
    "    def _tokenizer(self, x):\n",
    "        tokenizer = AutoTokenizer.from_pretrained('nbroad/ESG-BERT')\n",
    "        inference_encodings = tokenizer(x.to_list(), truncation=True, padding=True)\n",
    "        return inference_encodings\n",
    "    \n",
    "    def _get_dataset(self, inference_encodings):\n",
    "        dataset = EsgDataset(inference_encodings)\n",
    "        return dataset\n",
    "    \n",
    "    def _get_dataloader(self, inference) -> DataLoader:\n",
    "        inference_loader = DataLoader(inference, batch_size=self.HP['batch_size'], shuffle=True)\n",
    "        return inference_loader\n",
    "    \n",
    "    def _get_model(self) -> Tuple[EsgBert, torch.optim.Optimizer]:\n",
    "        model = EsgBert().to(device)\n",
    "        optim = AdamW(model.parameters(), lr=1e-5)\n",
    "        return model, optim\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, model: EsgBert, optim: torch.optim.Optimizer, inference_loader):\n",
    "        count = 0\n",
    "        model.eval()\n",
    "        # collect output\n",
    "        output_label = []\n",
    "        loop = tqdm(inference_loader, leave=True)\n",
    "        for batch_id, batch in enumerate(loop):\n",
    "            # reset\n",
    "            optim.zero_grad()\n",
    "            inputs = batch\n",
    "            input_ids = inputs['input_ids'].to(self.device)\n",
    "            attention_mask = inputs['attention_mask'].to(self.device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            #  collect output into list\n",
    "            outputs = outputs.cpu().tolist()\n",
    "            output_label.extend(outputs)\n",
    "            if batch_id % 50 == 0 and batch_id != 0:\n",
    "                print(f'Epoch {batch_id}, count is {count}')\n",
    "        self._gen_csv(output_label)\n",
    "        \n",
    "    def _gen_csv(self, output_label):\n",
    "        self.df['esgbert'] = output_label\n",
    "        self.df.to_csv(f'{self.CSV_OUTPUT}/{self.NATION}/test.csv', index=False)\n",
    "        \n",
    "    def main(self):\n",
    "        x = self._read_csv()\n",
    "        inference_encodings = self._tokenizer(x)\n",
    "        inference_dataset = self._get_dataset(inference_encodings)\n",
    "        inference_loader = self._get_dataloader(inference_dataset)\n",
    "        model, optim = self._get_model()\n",
    "        self.inference(model, optim, inference_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMETERS = {\n",
    "    'batch_size': 8,\n",
    "    'lr': 1e-5,\n",
    "    'epochs': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_SOURCE = '../../data/csv_source'\n",
    "CSV_OUTPUT = '../../data/csv_output'\n",
    "NATION = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VScodeProject\\ESG-GPT\\esg\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 53/78 [00:05<00:01, 17.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, count is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.03it/s]\n"
     ]
    }
   ],
   "source": [
    "esgbert = EsgBertPredict(CSV_SOURCE, CSV_OUTPUT, NATION, HYPER_PARAMETERS)\n",
    "esgbert.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
